### Introduction to Regression

#### What is Regression?

> Regression is the process of predicting a continuous value.

There are 2 types of variables:
- A dependent variable (X)
- One or more independent variable (Y)

#### What is a regression model?

We use regression to build regression estimation model; then the model is used to predict.

#### Types of regression models

- Simple Regression: One independent variable is used to estimate a dependent variable.
  - Simple Linear Regression
  - Simple Non-linear Regression

- Multiple Regression: More than one independent variable is used to estimate a dependent variable.
  - Multiple Linear Regression
  - Multiple Non-linear Regression

#### Regression algorithms

- Ordinal regression
- Poisson regression
- Fast forest quantile regression
- Linear, Polynomial, Lasso, Stepwise, Ridge regression
- Bayesian linear regression
- Neural network regression
- Decision forest regression
- Boosted decision tree regression
- KNN (K-nearest neighbors)

### Simple Linear Regression

#### Linear regression model representation

The fit line is shown as a polynomial. In a simple regression, the form of model would be: ![render.png](https://2.pik.vn/2022f4c0d7f9-3ab4-4399-9ae8-9b615febf9bf.png) <br />

#### How to find the best fit?

The difference between the actual value and the predicted value is called "residual error". <br />

The mean of all residual errors show how poorly the line fits with the whole data set. Mathematically it can be shown by the equation Mean Squared Error (MSE) <br />
![render.png](https://2.pik.vn/20223f8dbea2-286f-48a0-9e62-ba261c992df4.png) <br />

The objective of linear regression is to minimize MSE.

#### Estimating the parameters

![render.png](https://2.pik.vn/2022faa5cfd2-de65-4c04-8a2c-bf6b12e94830.png)
<br />
![tex2img.png](https://2.pik.vn/20229679076a-e573-44a7-b1ff-3092fd5be914.png)

#### Pros of linear regression

- Very fast
- No parameters tuning
- Easy to understand, and highly interpretable

### Model Evaluation in Regression Models

#### Model Evaluation approaches

- Train and Test on the Same Dataset
- Train/Test Split

#### Calculating the accuracy of a model

![render.png](https://2.pik.vn/20227e15f2b7-d61c-4a50-a115-cad20bcdfbb6.png)

#### Train and test on the same dataset

- Test-set is a portion of the train-set
- High "training accuracy"
- Low "out-of-sample accuracy"

#### What is training & out-of-sample accuracy?

Training accuracy is the percentage of correct predictions that the model makes when using the test dataset.

- High training accuracy isn't necessarily a good thing
- Result of over-fitting
  - Over-fit: the model is overly trained to the dataset, which may capture noise and produce a non-generalized model

Out-of-sample accuracy: 

- It's important that our models have a high out-of-sample accuracy
- One way to improve out-of-sample accuracy is to use another evaluation approach called train/test split

#### Train/Test split evaluation approach
- Mutally exclusive
- More accurate evaluation on out-of-sample accuracy
- Highly dependent on which datasets the data is trained and tested

#### K-fold cross-validation

- The original sample is randomly partitioned into K equal sized subsamples
- Of the K subsamples, a single subsample is retained as the validation data for testing the model, and the reamaining K-1 subsamples are used as training data
- Repeat K times

### Evaluation Metrics in Regression Models

Evaluation metrics are used to explain the performance of a model. <br />

We'll be mentioning number of model evaluation metrics, including Mean Absolute Error, Mean Squared Error, and Root Mean Squared Error.

#### What is an error of the model?

The error of the model is the difference between the data points and the trend line generated by the algorithm.

- Mean Absolute Error is the mean of the absolute value of the errors <br />
![render.png](https://2.pik.vn/202274805912-e3ec-4029-9403-1bfaba80dabd.png)

- Mean Squared Error is the mean of the squared error. It's more popular than MAE because the focus is geared more towards large errors <br />
![render.png](https://2.pik.vn/20223f8dbea2-286f-48a0-9e62-ba261c992df4.png)

- Root Mean Squared Error is the square root of MSE. It's one of the most popular of the evaluation metrics <br />
![render.png](https://2.pik.vn/202287e5fc68-eac6-467e-9c1a-356a5cbd2ff5.png)

- Relative absolute error aka Residual sum of square <br />
![render.png](https://2.pik.vn/2022613a52f4-8f6b-495b-93cd-331a986b95dd.png)

RSE is very similar to RAE, but is widely adopted by the DS community as it is used for calculating R-squared. R-squared is not an error per say but is a popular metric for the accuracy of your model. It represents how close the data values are to the fitted regression line. The higher the R-squared, the better the model fits your data. <br />
<p align="center">
  R-squared = 1 - RSE
</p>

### Multiple Linear Regression

#### Examples of multiple linear regression

- Independent variables effectiveness on prediction

- Predicting impacts of changes

#### Predicting continuous values with multiple linear regression

![render.png](https://2.pik.vn/2022a6756876-19ab-40b7-93f4-a14ca59e4201.png) <br />
Vector form: ![render.png](https://2.pik.vn/2022aed710eb-a9f0-4de7-bb3b-98a21fa0b63e.png)

#### Estimating multiple linear regression parameters

- Ordinary Least Squares: try to estimate the values of the coefficients by minimizing the mean square error
  - Linear algebra operations
  - Take a long time for large datasets

- An optimization algorithm
  - Gradient Descent: starts optimization with random values for each coefficient, then calculates the errors and tries to minimize it through y's changing of the coefficients in multiple iterations
  - Proper approach if you have a very large dataset

### Non-Linear Regression

#### Different types of regression

- Linear Regression

- Quadratic Regression

- Cubic Regression

- ...

In essence, we can call all of these polynomial regression.

#### What is polynomial regression?

Polynomial regression fits a curve line to your data. <br />

A polynomial regression model can be transformed into linear regression model. Therefore, polynomial regression is considered to be a special case of traditional multiple linear regression. <br />

Polynomial regression models can fit using the model of least squares. <br />

Least squares is a method for estimating the unknown parameters in a linear regression model by minimizing the sum of the squares of the differences between y and y_hat <br />

#### What is non-linear regression?

Non-linear regression is a method to model non-linear relationship between the dependent variable and a set of independent variables. <br />

y_hat must be a non-linear function of the parameter theta, not necessarily the features x.

#### Linear vs non-linear regression

- How can I know if a problem is linear or non-linear regression?
  - Inspect visually
  - Based on accuracy: use non-linear regression instead of linear regression when we cannot accurately model the relationship with linear parameters

- How should I model my data, if it displays non-linear on a scatter plot?
  - Polynomial regression
  - Non-linear regression
  - Transform your data